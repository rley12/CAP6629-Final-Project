{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824459a8-08ad-4aa5-bb77-f257480dd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11a998-2704-4156-98e5-a993fc286c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.999\n",
    "alpha = 1\n",
    "door = -4\n",
    "wall = -200\n",
    "home = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44411152-3315-4994-acd5-5c61f6f51dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardsTable = np.array([\n",
    "    [None,-1,None,None,wall,None,None,None,None,None,None,None,None,None,None,None],\n",
    "    [-1,None,-1,None,None,door,None,None,None,None,None,None,None,None,None,None],\n",
    "    [None,-1,None,-1,None,None,wall,None,None,None,None,None,None,None,None,None],\n",
    "    [None,None,-1,None,None,None,None,-1,None,None,None,None,None,None,None,None],\n",
    "    [-1,None,None,None,None,door,None,None,-1,None,None,None,None,None,None,None],\n",
    "    [None,-1,None,None,wall,None,wall,None,None,-1,None,None,None,None,None,None],\n",
    "    [None,None,-1,None,None,door,None,-1,None,None,-1,None,None,None,None,None,None],\n",
    "    [None,None,None,-1,None,None,wall,None,None,None,None,-1,None,None,None,None],\n",
    "    [None,None,None,None,wall,None,None,None,None,-1,None,None,-1,None,None,None],\n",
    "    [None,None,None,None,None,door,None,None,-1,None,-1,None,None,home,None,None],\n",
    "    [None,None,None,None,None,None,wall,None,None,-1,None,-1,None,None,-1,None],\n",
    "    [None,None,None,None,None,None,None,-1,None,None,-1,None,None,None,None,-1],\n",
    "    [None,None,None,None,None,None,None,None,-1,None,None,None,None,home,None,None],\n",
    "    [None,None,None,None,None,None,None,None,None,-1,None,None,-1,None,-1,None],\n",
    "    [None,None,None,None,None,None,None,None,None,None,-1,None,None,home,None,-1],\n",
    "    [None,None,None,None,None,None,None,None,None,None,None,-1,None,None,-1,None]\n",
    "    ], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4308b8-5924-4ef7-92e2-f5167b6ddbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinatesToStates = {\n",
    "    'A1' : 0,\n",
    "    'B1' : 1,\n",
    "    'C1' : 2,\n",
    "    'D1' : 3,\n",
    "    'A2' : 4,\n",
    "    'B2' : 5,\n",
    "    'C2' : 6,\n",
    "    'D2' : 7,\n",
    "    'A3' : 8,\n",
    "    'B3' : 9,\n",
    "    'C3' : 10,\n",
    "    'D3' : 11,\n",
    "    'A4' : 12,\n",
    "    'B4' : 13,\n",
    "    'C4' : 14,\n",
    "    'D4' : 15,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f3574-2269-464b-b538-5c16b75f21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "possibleActions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892a4ac9-da64-4c73-9dc7-1062b5f8fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateToCoordinates = {state: coordinates for coordinates, state in coordinatesToStates.items()}\n",
    "\n",
    "def policy(stateT, legalActions, QValues, method, epsilon):\n",
    "    if method == 'QLearning':\n",
    "        return np.random.choice(legalActions)\n",
    "    if method == 'SARSA':\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(legalActions)\n",
    "        else:\n",
    "            bestAction = legalActions[0]\n",
    "            for index in legalActions:\n",
    "                if QValues[stateT][index] >= QValues[stateT][bestAction]:\n",
    "                    bestAction = index\n",
    "            return bestAction\n",
    "\n",
    "def legalActionChecker(index, rewards, state):\n",
    "    legalActions = []\n",
    "    i = 0\n",
    "    while i <= index:\n",
    "        if rewards[state][i] != None:\n",
    "            legalActions.append(i)\n",
    "        i+=1\n",
    "    return legalActions\n",
    "\n",
    "def argmaxAction(legalAction, QTable, stateT):\n",
    "    maxQAction = legalAction[0]\n",
    "    for index in legalAction:\n",
    "        if QTable[stateT][index] >= QTable[stateT][maxQAction]:\n",
    "            maxQAction = index\n",
    "    return maxQAction\n",
    "    \n",
    "def path(QValues, startCoord = 'A1', endCoord = 'B4'):\n",
    "    path = [startCoord]\n",
    "    reward = 0\n",
    "    nextCoord = startCoord\n",
    "    while (nextCoord != endCoord):\n",
    "        startState = coordinatesToStates[startCoord]\n",
    "        legalActions = legalActionChecker(15, rewardsTable, startState)\n",
    "        nextState = argmaxAction(legalActions, QValues, startState)\n",
    "        nextCoord = stateToCoordinates[nextState]\n",
    "        reward += rewardsTable[startState][nextState]\n",
    "        path.append(nextCoord)\n",
    "        startCoord = nextCoord\n",
    "    return {'Reward': reward, 'Path':path}\n",
    "\n",
    "def qLearning(episodeCount, method, epsilon):\n",
    "    currentEpisode = 0\n",
    "    QValues = np.zeros((16, 16))\n",
    "\n",
    "    while currentEpisode < episodeCount:\n",
    "        stateT = 0\n",
    "        stateNext = int\n",
    "        legalActions = []\n",
    "        reward = 0\n",
    "        rewardArr = []\n",
    "        i = 0\n",
    "        #While haven't reached the terminal state\n",
    "        while stateNext != 13:\n",
    "            legalActions = legalActionChecker(15,rewardsTable,stateT)\n",
    "            #print('Step',i, 'is ', stateT)\n",
    "            \n",
    "            stateNext= policy(stateT, legalActions,QValues,method,epsilon)\n",
    "            reward += rewardsTable[stateT][stateNext]\n",
    "            bestAction = argmaxAction(legalActionChecker(15,rewardsTable,stateNext), QValues, stateNext)\n",
    "            TD = rewardsTable[stateT][stateNext] + gamma * QValues[stateNext][bestAction] - QValues[stateT][stateNext]\n",
    "            QValues[stateT][stateNext] += alpha*TD \n",
    "            stateT=stateNext\n",
    "            legalActions = []\n",
    "            i = 0\n",
    "        rewardArr.append(path(QValues)['Reward'])\n",
    "        reward = 0\n",
    "        print(rewardArr)\n",
    "        currentEpisode += 1\n",
    "    return {'QValues': QValues, 'rewardArr': rewardArr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da710a8e-1b56-4b05-8f69-bbc0f17719d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateToCoordinates = {state: coordinates for coordinates, state in coordinatesToStates.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c325c8a-b486-4c18-aa43-97eab098c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(qLearning(1,'QLearning', 0.5).astype(int), columns=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n",
    "\n",
    "qlearn = qLearning(100,'QLearning', 0.5)\n",
    "#print(qLearning(15,'SARSA', 0.5).astype(int))\n",
    "print(path(qlearn['QValues']))\n",
    "plt.plot(qlearn['rewardArr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124c960-5f4b-4dfe-ac77-eb35b9505839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(qLearning(1,'SARSA', 0.4), columns=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
